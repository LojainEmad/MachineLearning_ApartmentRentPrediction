# -*- coding: utf-8 -*-
"""PatternMS1WorkingFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RUS66J4dvDzF47Md8H8mtCK1KvUmmWY2
"""

!pip install nltk
!pip install category_encoders
!pip install --upgrade sklearn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

from sklearn.pipeline import make_pipeline

from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import r2_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from category_encoders import CatBoostEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import export_text
from sklearn.model_selection import cross_val_score

from scipy import stats

from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut

import nltk
from nltk import download
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('wordnet')
download('punkt')
nltk.download("stopwords")

data = pd.read_csv('ApartmentRentPrediction.csv')
#data = pd.read_csv('updatedApartmentRentPrediction.csv')
data.info()

data.describe()

data.head()

print(data.isnull().sum())

data.duplicated().sum()

data['address'].nunique()

data['address'].value_counts()

#replacing the null values:

#replacing with none as null here has meaning of not having those features
data['pets_allowed'].fillna('none', inplace=True)
data['amenities'].fillna('none', inplace=True)

#replacing with mode even if numeric as it is descrete and not continuous
data['bathrooms'] = data['bathrooms'].fillna(data['bathrooms'].mode().iloc[0])
data['bedrooms'] = data['bedrooms'].fillna(data['bedrooms'].mode().iloc[0])
#replacing float with int for logical reasons and data integrity
data['bathrooms'] = data['bathrooms'].astype(int)
data['bedrooms'] = data['bathrooms'].astype(int)

#replacing categorical data with mode
#data['cityname'] = data['cityname'].fillna(data['cityname'].mode().iloc[0])
#data['state'] = data['state'].fillna(data['state'].mode().iloc[0])

#replacing continuous data with mean
data['latitude'] = data['latitude'].fillna(data['latitude'].mean())
data['longitude'] = data['longitude'].fillna(data['longitude'].mean())

#doesn't make sense to me: the number of unique addresses is large and the highest repetition is only 3 -> replacing all the null values with it will break the integrity of the data and have 3000 listing with the same address

#data['address'] = data['address'].fillna(data['address'].mode().iloc[0])
#data['address'].fillna('Unknown', inplace=True)

print(data.isnull().sum())

geolocator = Nominatim(user_agent="reverse_geocoding", timeout=90)

def reverse_geocode(latitude, longitude):
    max_retries = 3
    retries = 0
    while retries < max_retries:
        try:
            location = geolocator.reverse((latitude, longitude), language='en')
            return location.address
        except GeocoderTimedOut:
            retries += 1
            print(f"Geocoder timed out. Retrying... Attempt {retries}/{max_retries}")
    print("Max retries exceeded. Unable to geocode.")
    return None

null_address_rows = data[data['address'].isnull()]

batch_size = 100
for i in range(0, len(null_address_rows), batch_size):
    batch_data = null_address_rows.iloc[i:i+batch_size]
    for index, row in batch_data.iterrows():
        latitude = row['latitude']
        longitude = row['longitude']
        address = reverse_geocode(latitude, longitude)
        if address:
            data.at[index, 'address'] = address
            print(f"New address added at index {index}: {address}")

# preprocessing  body coulmn

pattern = r'is located at\s*.*?,(.*?),'

# Extract city names
city_name_from_body = data['body'].str.extract(pattern)
city_name_from_body = np.array(city_name_from_body)
city_name_from_body_series = pd.Series(city_name_from_body.flatten())

#print(city_name_from_body_series)
print(data['cityname'].isnull().sum())
data['cityname'].fillna(city_name_from_body_series, inplace = True)
print(data['cityname'].isnull().sum())
data['cityname'] = data['cityname'].fillna(data['cityname'].mode().iloc[0])

#Extract state
pat = r'(\b[A-Z]{2}\b)'
state_from_body = data['body'].str.extractall(pat)
state_from_body = np.array(state_from_body)
state_from_body_series = pd.Series(state_from_body.flatten())
data['state'].fillna(state_from_body_series, inplace=True)

#print(state_from_body_series.notnull())
print(data['state'].isnull().sum())
data['state'].fillna(data['state'].mode().iloc[0], inplace=True)
print(data['state'][187])

print(data.isnull().sum())

data['address'].nunique()

data['address'].value_counts() #compare the difference before and after replacing null with mode to understand note

data['pets_allowed'].nunique()

data['pets_allowed'].value_counts()

data['amenities'].nunique()

data['cityname'].nunique()

data['cityname'].value_counts()

data['has_photo'].nunique()

data['has_photo'].value_counts()

data['price_type'].nunique()

data['price_type'].value_counts()

data['currency'].nunique()

data['category'].nunique()

data['category'].value_counts()

data['fee'].value_counts()

data['state'].nunique()

data['state'].value_counts()

data['source'].nunique()

data['source'].value_counts()

data['body'].nunique() #almost each single body is different

data['title'].nunique() #almot all are different

#:Label Encoding
columns_to_label_encode = ['price_type', 'currency', 'category', 'fee']
le = LabelEncoder()
data['price_type'] = le.fit_transform(data['price_type'])
data['currency'] = le.fit_transform(data['currency'])
data['category'] = le.fit_transform(data['category'])
data['fee'] = le.fit_transform(data['fee'])

data['price_display'] = data['price_display'].str.replace('[^\d]', '', regex=True)

data['price_display'] = pd.to_numeric(data['price_display'], errors='coerce')

data['price_display'].fillna(0, inplace=True)

data['price_display'] = data['price_display'].astype(int)

print(data['price_display'])
print(data['price'])

print(data['price_type'])

data['category'].value_counts()

columns_to_one_hot_encoded = ['has_photo', 'source']

#grouping the other sources below the top two into single category 'Other'

source_counts = data['source'].value_counts()

top_two = source_counts.head(2).index.tolist()

mask = ~data['source'].isin(top_two)

data.loc[mask, 'source'] = 'Other'

encoded_dfs = []
for column in columns_to_one_hot_encoded:

    one_hot_encoded = pd.get_dummies(data[column])
    one_hot_encoded = one_hot_encoded.astype(int)
    encoded_dfs.append(one_hot_encoded)

encoded_data = pd.concat([data] + encoded_dfs, axis=1)
encoded_data.drop(columns=columns_to_one_hot_encoded, axis=1, inplace=True)
data = encoded_data


one_hot_encoded1 = data['pets_allowed'].str.get_dummies(sep=',')


data = pd.concat([data, one_hot_encoded1], axis=1)
data.drop('pets_allowed', axis=1, inplace=True)


one_hot_encoded2 = data['amenities'].str.get_dummies(sep=',')

data = pd.concat([data, one_hot_encoded2], axis=1)
data.drop('amenities', axis=1, inplace=True)

print(data.head())

for column in data.columns:
    print(column)

print(data['price_display'].value_counts())
print(data.head())

# Define regular expressions to extract data
bedroom_pattern = re.compile(r'(one|two|three|four|five|six|seven|eight|nine|ten)\s*BR', re.IGNORECASE)
address_pattern = re.compile(r'\d+(\s+\w+)+')

bedrooms_from_title = []
addresses_from_title = []

for title in data['title']:
    # Extract number of bedrooms
    bedroom_match = bedroom_pattern.search(title)
    if bedroom_match:
        bedrooms_from_title.append(bedroom_match.group())
    else:
        bedrooms_from_title.append(None)

    # Extract address
    address_match = address_pattern.search(title)
    if address_match:
        addresses_from_title.append(address_match.group())
    else:
        addresses_from_title.append(None)

# Create a new DataFrame with the extracted data
extracted_data = pd.DataFrame({
    'Bedrooms From Title': bedrooms_from_title,
    'Address From Title': addresses_from_title
})

#data.drop(columns=['title'], inplace=True)

#data = pd.concat([data, extracted_data], axis=1)
# Display the first 20 rows of the extracted data DataFrame
print(extracted_data.head(20))

stop_words = set(stopwords.words('english'))

def preprocess(text):
    tokens = word_tokenize(text)
    filtered_tokens = [word.lower() for word in tokens if word.isalpha()]
    filtered_tokens = [word for word in filtered_tokens if word not in stop_words]
    return " ".join(filtered_tokens)

preprocessed_corpus = [preprocess(row) for row in data['body']]

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_corpus)
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

print(tfidf_df)

word_scores = tfidf_df.sum()
sorted_words = word_scores.sort_values(ascending=False)
N = 60
top_n_words = sorted_words.head(N)
print("Top", N, "most important words:")
print(top_n_words)

word = 'two'
data['two'] = data['body'].apply(lambda x: 1 if word in x else 0)
data['two']

word = 'community'
data['community'] = data['body'].apply(lambda x: 1 if word in x else 0)
data['community']

word = 'features'
data['features'] = data['body'].apply(lambda x: 1 if word in x else 0)
data['features']

word = 'include'
data['include'] = data['body'].apply(lambda x: 1 if word in x else 0)
data['include']

encoder = CatBoostEncoder()

columns_to_target_encode = ['state', 'cityname','body','title', 'address']

encoder.fit(data[columns_to_target_encode], data['price_display'])

encoded_data = encoder.transform(data[columns_to_target_encode])

data[columns_to_target_encode] = encoded_data

print(data['cityname'])

#outlier detection:
# z_scores = np.abs(stats.zscore(data))
# threshold = 4
# outlier_rows = np.any(z_scores > threshold, axis=1)
# data[outlier_rows].shape

# data = data[~outlier_rows]

# columns_to_normalize = ['id', 'title', 'body','square_feet','bathrooms', 'bedrooms', 'latitude', 'longitude', 'time', 'price', 'address', 'cityname', 'state']
# scaler = MinMaxScaler()
# data[columns_to_normalize] = scaler.fit_transform(data[columns_to_normalize])
# data.head()

#print(data['state'])
#data = pd.DataFrame(data)
print(data.head())

X = data.drop('price', axis=1)
X = X.drop('price_display', axis=1)
Y = data['price_display']

print(X.info())
print(Y.info())

corr_with_price = data.corrwith(data['price_display'])

corr_with_price

corr = data.corr()
top_features = corr['price_display'].abs() > 0.3 #& (corr['price_display'].abs() < 0.9)
plt.subplots(figsize=(12, 8))
top_corr =  X.loc[:, top_features].corr()
sns.heatmap(top_corr, annot=True)

X = X.loc[:, top_features]
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20,shuffle=True,random_state=10)
print(X_train)
print(y_train)
X_train.info()

# Scaling the data
#y_train= y_train.values.reshape(-1,1)
#y_test= y_test.values.reshape(-1,1)

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.fit_transform(X_test)
#y_train = sc_X.fit_transform(y_train)
#y_test = sc_y.fit_transform(y_test)

X_train

#multilevel linear regression - train error
multi_model = linear_model.LinearRegression()
multi_model.fit(X_train, y_train)
prediction = multi_model.predict(X_train)
print('Mean Square Error', metrics.mean_squared_error(y_train, prediction))
r2 = r2_score(y_train, prediction)
acc = r2 * 100
print('Model Accuracy', acc)

#multilevel linear regression - test error
pred = multi_model.predict(X_test)
print('Mean Square Error', metrics.mean_squared_error(y_test, pred))
r2 = r2_score(y_test, pred)
acc = r2 * 100
print('Model Accuracy', acc)

from sklearn.decomposition import PCA

pipeline = make_pipeline(StandardScaler(), PCA(n_components=2), LinearRegression())

pipeline.fit(X_train, y_train)

prediction = pipeline.predict(X_train)

# Plotting
plt.scatter(y_train, prediction, label='Actual vs Predicted')
plt.xlabel('Actual')
plt.ylabel('Predicted')

# Plot the regression line
plt.plot(np.unique(y_train), np.poly1d(np.polyfit(y_train, prediction, 1))(np.unique(y_train)), color='red', label='Regression Line')

plt.title('MultiLinear')
plt.legend()
plt.show()

#hyperparameter tuning
param_dist = {'polynomialfeatures__degree': randint(1, 10)}

poly_reg = make_pipeline(PolynomialFeatures(), LinearRegression())

random_search = RandomizedSearchCV(poly_reg, param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=10)

random_search.fit(X_train, y_train)

best_degree = random_search.best_params_['polynomialfeatures__degree']
print("Best degree value:", best_degree)

#polynomial regression - train
poly_features = PolynomialFeatures(degree=2)
X_train_poly = poly_features.fit_transform(X_train)
poly_model = linear_model.LinearRegression()
poly_model.fit(X_train_poly, y_train)
y_train_predicted = poly_model.predict(X_train_poly)
ypred=poly_model.predict(poly_features.transform(X_train))
print(ypred.size)

#polynomial regression - test
y_test_pred = poly_model.predict(poly_features.fit_transform(X_test))
print(y_test_pred.size)

#polynomial regression - train
print('Mean Square Error', metrics.mean_squared_error(y_train, ypred))
r2 = r2_score(y_train, ypred)
acc = r2 * 100
print('Model Accuracy', acc)

#polynomial regression - test
print('Mean Square Error', metrics.mean_squared_error(y_test, y_test_pred))
r2 = r2_score(y_test, y_test_pred)*100
print('Model Accuracy', r2)

#Visualize the Polynomial Regression line in 2D
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)


poly_features = PolynomialFeatures(degree=2)
X_train_pca_poly = poly_features.fit_transform(X_train_pca)
poly_model = LinearRegression()
poly_model.fit(X_train_pca_poly, y_train)


plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

x_line = np.linspace(X_train_pca[:, 0].min(), X_train_pca[:, 0].max(), 100)
y_line = np.linspace(X_train_pca[:, 1].min(), X_train_pca[:, 1].max(), 100)
x_line, y_line = np.meshgrid(x_line, y_line)
xy_line = np.column_stack((x_line.ravel(), y_line.ravel()))
xy_line_poly = poly_features.transform(xy_line)
z_line = poly_model.predict(xy_line_poly)
z_line = z_line.reshape(x_line.shape)

plt.contour(x_line, y_line, z_line, levels=20, cmap='RdGy')
plt.colorbar(label='Predicted y')
plt.title('Polynomial Regression in 2D (PCA)')
plt.show()

#Random Forest Regression
regressor = RandomForestRegressor(n_estimators=300, max_depth=10, min_samples_split=30, n_jobs=-1, random_state=0, oob_score=True)
regressor.fit(X_train, y_train)

oob_score = regressor.oob_score_
print(f'Out-of-Bag Score: {oob_score}')

predictions = regressor.predict(X_test)

mse = metrics.mean_squared_error(y_test, predictions)
print(f'Mean Squared Error: {mse}')

r2 = r2_score(y_test, predictions)*100
print(f'Model Accuracy: {r2}')

#Decision Tree Regression

regressor = DecisionTreeRegressor(min_samples_split=30, max_depth=10)
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)

print('Mean Square Error', metrics.mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)*100
print('Model Accuracy', r2)

from sklearn.ensemble import GradientBoostingRegressor

model = GradientBoostingRegressor(n_estimators=300, min_samples_split=20)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print('Mean Square Error', metrics.mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)*100
print('Model Accuracy', r2)