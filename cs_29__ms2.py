# -*- coding: utf-8 -*-
"""cs_29 _MS2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GErS1bpyJ3mR9MVJjFXAPN19C2hxGIMj
"""

!pip install nltk
!pip install category_encoders
!pip install --upgrade sklearn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
import time
from sklearn import linear_model
#from sklearn.linear_model import LinearRegression
#from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

from sklearn.pipeline import make_pipeline

from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import r2_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from category_encoders import CatBoostEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import export_text
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import f_classif
import pickle

from scipy import stats

from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut

import nltk
from nltk import download
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('wordnet')
download('punkt')
nltk.download("stopwords")

data = pd.read_csv('/content/ApartmentRentPrediction_Milestone2.csv')
time.time()

#data = pd.read_csv('updatedApartmentRentPrediction.csv')
X = data.drop('RentCategory',axis=1)
Y = data['RentCategory']
data.info()
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, shuffle=True, random_state=10)

# Print training data and information
print("X_train:")
print(X_train)
print("y_train:")
print(y_train)
print("X_train info:")
print(X_train.info())
print("y_train info:")
print(y_train.info())

duplicates = X_train.duplicated()
print("Are there any duplicate rows?", duplicates.any())
print("Number of duplicate rows:", duplicates.sum())
print(X_train[duplicates])

X_train.describe()

X_train.head()

print(X_train.isnull().sum())

X_train['address'].nunique()

X_train['address'].value_counts()

#replacing the null values:

#replacing with none as null here has meaning of not having those features
X_train['pets_allowed'].fillna('none', inplace=True)
X_train['amenities'].fillna('none', inplace=True)

#replacing with mode even if numeric as it is descrete and not continuous
X_train['bathrooms'] = X_train['bathrooms'].fillna(X_train['bathrooms'].mode().iloc[0])
X_train['bedrooms'] = X_train['bedrooms'].fillna(X_train['bedrooms'].mode().iloc[0])

#replacing float with int for logical reasons and data integrity
X_train['bathrooms'] = X_train['bathrooms'].astype(int)
X_train['bedrooms'] = X_train['bathrooms'].astype(int)

#replacing continuous data with mean
X_train['latitude'] = X_train['latitude'].fillna(X_train['latitude'].mean())
X_train['longitude'] = X_train['longitude'].fillna(X_train['longitude'].mean())

print(X_train.isnull().sum())

def reverse_geocode(latitude, longitude):
    geolocator = Nominatim(user_agent="reverse_geocoding", timeout=90)
    max_retries = 3
    retries = 0
    while retries < max_retries:
        try:
            location = geolocator.reverse((latitude, longitude), language='en')
            return location.address
        except GeocoderTimedOut:
            retries += 1
            print(f"Geocoder timed out. Retrying... Attempt {retries}/{max_retries}")
    print("Max retries exceeded. Unable to geocode.")
    return None

null_address_rows = X_train[X_train['address'].isnull()]

batch_size = 100
for i in range(0, len(null_address_rows), batch_size):
    batch_data = null_address_rows.iloc[i:i+batch_size]
    for index, row in batch_data.iterrows():
        latitude = row['latitude']
        longitude = row['longitude']
        address = reverse_geocode(latitude, longitude)
        if address:
            X_train.at[index, 'address'] = address
            print(f"New address added at index {index}: {address}")

# preprocessing  body coulmn

def preprocessing_body(X_train):
  pattern = r'is located at\s*.*?,(.*?),'

  # Extract city names
  city_name_from_body = X_train['body'].str.extract(pattern)
  city_name_from_body = np.array(city_name_from_body)
  city_name_from_body_series = pd.Series(city_name_from_body.flatten())

  #print(city_name_from_body_series)
  #print(X_train['cityname'].isnull().sum())
  X_train['cityname'].fillna(city_name_from_body_series, inplace = True)
  #print(X_train['cityname'].isnull().sum())
  X_train['cityname'] = X_train['cityname'].fillna(X_train['cityname'].mode().iloc[0])
  #print(X_train['cityname'].isnull().sum())

  #Extract state
  pat = r'(\b[A-Z]{2}\b)'
  state_from_body = X_train['body'].str.extractall(pat)
  state_from_body = np.array(state_from_body)
  state_from_body_series = pd.Series(state_from_body.flatten())
  X_train['state'].fillna(state_from_body_series, inplace=True)

  #print(state_from_body_series.notnull())
  #print(X_train['state'].isnull().sum())
  X_train['state'].fillna(X_train['state'].mode().iloc[0], inplace=True)
  #print(X_train['state'].isnull().sum())

preprocessing_body(X_train)

print(X_train.isnull().sum())

bathrooms_mode = X_train['bathrooms'].mode().iloc[0]
bedrooms_mode = X_train['bedrooms'].mode().iloc[0]
longitude_mean = X_train['longitude'].mean()
latitude_mean = X_train['latitude'].mean()
cityname_mode = X_train['cityname'].mode().iloc[0]
state_mode = X_train['state'].mode().iloc[0]

X_train['address'].nunique()

X_train['address'].value_counts() #compare the difference before and after replacing null with mode to understand note

X_train['pets_allowed'].nunique()

X_train['pets_allowed'].value_counts()

X_train['amenities'].nunique()

X_train['cityname'].nunique()

X_train['cityname'].value_counts()

X_train['has_photo'].nunique()

X_train['has_photo'].value_counts()

X_train['price_type'].nunique()

X_train['price_type'].value_counts()

X_train['currency'].nunique()

X_train['category'].nunique()

X_train['category'].value_counts()

X_train['fee'].value_counts()

X_train['state'].nunique()

X_train['state'].value_counts()

X_train['source'].nunique()

X_train['source'].value_counts()

X_train['body'].nunique() #almost each single body is different

X_train['title'].nunique() #almot all are different

# Define regular expressions to extract data from title column
bedroom_pattern = re.compile(r'(one|two|three|four|five|six|seven|eight|nine|ten)\s*BR', re.IGNORECASE)
address_pattern = re.compile(r'\d+(\s+\w+)+')

bedrooms_from_title = []
addresses_from_title = []

for title in data['title']:
    # Extract number of bedrooms
    bedroom_match = bedroom_pattern.search(title)
    if bedroom_match:
        bedrooms_from_title.append(bedroom_match.group())
    else:
        bedrooms_from_title.append(None)

    # Extract address
    address_match = address_pattern.search(title)
    if address_match:
        addresses_from_title.append(address_match.group())
    else:
        addresses_from_title.append(None)

# Create a new DataFrame with the extracted data
extracted_data = pd.DataFrame({
    'Bedrooms From Title': bedrooms_from_title,
    'Address From Title': addresses_from_title
})

#data.drop(columns=['title'], inplace=True)

#data = pd.concat([data, extracted_data], axis=1)
# Display the first 20 rows of the extracted data DataFrame
print(extracted_data.head(20))

from sklearn.preprocessing import LabelEncoder
import numpy as np

class SafeLabelEncoder(LabelEncoder):
    def transform(self, y):
        y = np.array(y)
        # Fit label encoder and return encoded labels
        encoded_labels = []
        unseen_labels_count = 0
        for x in y:
            if x in self.classes_:
                encoded_labels.append(np.where(self.classes_ == x)[0][0])
            else:
                # If unseen label, assign a new label by incrementing the maximum label value
                unseen_labels_count += 1
                encoded_labels.append(len(self.classes_) + unseen_labels_count - 1)
        return np.array(encoded_labels)

#:Label Encoding # we should apply label encoding in  the targt "rent type "
print(y_train)
label_encoders = {}
columns_to_label_encode = ['price_type', 'currency', 'category', 'fee']

for column in columns_to_label_encode:
    label_encoders[column] = SafeLabelEncoder()
    X_train[column] = label_encoders[column].fit_transform(X_train[column])


y_encoder = SafeLabelEncoder()
y_train = y_encoder.fit_transform(y_train)
print(y_train)

print(X_train.info())

print(X_train['price_type'])

X_train['category'].value_counts()

# hot encoding
def encoding_categorical_data(X_train):
  columns_to_one_hot_encoded = ['has_photo', 'source']

  #grouping the other sources below the top two into single category 'Other'

  source_counts = X_train['source'].value_counts()

  top_two = source_counts.head(2).index.tolist()

  mask = ~X_train['source'].isin(top_two)

  X_train.loc[mask, 'source'] = 'Other'

  encoded_dfs = []
  for column in columns_to_one_hot_encoded:

      one_hot_encoded = pd.get_dummies(X_train[column])
      one_hot_encoded = one_hot_encoded.astype(int)
      encoded_dfs.append(one_hot_encoded)

  encoded_data = pd.concat([X_train] + encoded_dfs, axis=1)
  encoded_data.drop(columns=columns_to_one_hot_encoded, axis=1, inplace=True)
  X_train = encoded_data


  one_hot_encoded1 = X_train['pets_allowed'].str.get_dummies(sep=',')


  X_train = pd.concat([X_train, one_hot_encoded1], axis=1)
  X_train.drop('pets_allowed', axis=1, inplace=True)


  one_hot_encoded2 = X_train['amenities'].str.get_dummies(sep=',')

  X_train = pd.concat([X_train, one_hot_encoded2], axis=1)
  X_train.drop('amenities', axis=1, inplace=True)

  print(X_train.info())

  return X_train


X_train = encoding_categorical_data(X_train)

print(X_train.info())

for column in X_train.columns:
    print(column)

y_train = pd.DataFrame(y_train, columns=['RentCategory'])

# Now you can use value_counts() method
print(y_train['RentCategory'].value_counts())

X_train['body'] = X_train['body'].astype(str)
stop_words = set(stopwords.words('english'))

def preprocess(text):
    tokens = word_tokenize(text)
    filtered_tokens = [word.lower() for word in tokens if word.isalpha()]
    filtered_tokens = [word for word in filtered_tokens if word not in stop_words]
    return " ".join(filtered_tokens)

preprocessed_corpus = [preprocess(row) for row in X_train['body']]

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_corpus)
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

print(tfidf_df)

word_scores = tfidf_df.sum()
sorted_words = word_scores.sort_values(ascending=False)
N = 60
top_n_words = sorted_words.head(N)
print("Top", N, "most important words:")
print(top_n_words)

word = 'available'
X_train['available'] = X_train['body'].apply(lambda x: 1 if word in x else 0)
X_train['available']

word = 'community'
X_train['community'] = X_train['body'].apply(lambda x: 1 if word in x else 0)
X_train['community']

word = 'features'
X_train['features'] = X_train['body'].apply(lambda x: 1 if word in x else 0)
X_train['features']

word = 'include'
X_train['include'] = X_train['body'].apply(lambda x: 1 if word in x else 0)
X_train['include']

import pandas as pd
from category_encoders import CatBoostEncoder

# Reset indexes of X_train and y_train
X_train.reset_index(drop=True, inplace=True)
y_train.reset_index(drop=True, inplace=True)

cat_encoder = CatBoostEncoder()

columns_to_target_encode = ['state', 'cityname', 'body', 'title', 'address']

cat_encoder.fit(X_train[columns_to_target_encode], y_train)

encoded_data = cat_encoder.transform(X_train[columns_to_target_encode])

X_train[columns_to_target_encode] = encoded_data

print(X_train['cityname'])

print(X_train.columns)
print(y_train)

print(X_train.info())

X_train.drop('available', axis=1, inplace=True)
X_train.drop('community', axis=1, inplace=True)
X_train.drop('features', axis=1, inplace=True)
X_train.drop('include', axis=1, inplace=True)

# applyin Anova for fearure selection
k = 5
selector = SelectKBest(score_func=f_classif, k=k)
selector.fit(X_train, y_train)
X_selected = selector.transform(X_train)

# Get the indices of the selected features
selected_feature_indices = selector.get_support(indices=True)
selected_feature_names = X_train.columns[selected_feature_indices]

print("Selected Features:", selected_feature_names)

# applying chi_squared for feature_selection


# import pandas as pd
# from sklearn.feature_selection import SelectKBest, chi2
# from sklearn.model_selection import train_test_split

# # Assume `X` is your dataframe of input features and `Y` is the target categorical variable

# # Convert numerical features to categorical if necessary
# # For example, if 'X' contains any continuous variables that you believe categorically represent the data:
# # X['feature_name'] = pd.cut(X['feature_name'], bins=3, labels=["Low", "Medium", "High"])

# # Feature selection using Chi-squared test
# k = 5  # Number of top features to select
# selector = SelectKBest(score_func=chi2, k=k)

# # Fit the selector to the data
# X_selected = selector.fit_transform(X, Y)

# # Get the indices of selected features
# selected_features_indices = selector.get_support(indices=True)

# # Get the names of the selected features
# selected_features_names = X.columns[selected_features_indices]
# print("Selected Features:", selected_features_names)

# # Use selected features for further analysis
# X_selected_df = X[selected_features_names]

# # Split the dataset
# X_train, X_test, y_train, y_test = train_test_split(X_selected_df, Y, test_size=0.20, shuffle=True, random_state=10)

# # Proceed with your analysis using X_train and y_train

X_train = X_train[selected_feature_names]
# print(X_train)
# print(X_train.isnull().sum())
# X_test = X_test[selected_feature_names]
# print(X_test.isnull().sum())

#appling scaleing
X_train.info()
scaler = StandardScaler()
scaler.fit(X_train)
X_scaled = scaler.transform(X_train)
X_train = pd.DataFrame(X_scaled, columns=X_train.columns)
X_train.info()

from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression

lin_model = LogisticRegression()
#solver='lbfgs', max_iter=1000
t1= time.time()
lin_model.fit(X_train, y_train)
y_pred = lin_model.predict(X_train)
accuracy = accuracy_score(y_train, y_pred)
print("Linear Regression Accuracy:", accuracy)
print("Training time 1:", time.time()-t1)

svm_model = svm.SVC(kernel='linear', C=1)
t2 = time.time()
svm_model.fit(X_train, y_train)
y_pred = svm_model.predict(X_train)
accuracy = accuracy_score(y_train, y_pred)
print("Linear SVM Accuracy:", accuracy)
print("Training time 2:", time.time()-t2)

poly_svm = svm.SVC(kernel='poly',degree=3, C=1)
t3 = time.time()
poly_svm.fit(X_train, y_train)
y_pred = poly_svm.predict(X_train)
accuracy = accuracy_score(y_train, y_pred)
print("poly SVM Accuracy:", accuracy)
print("Training time 3:", time.time()-t3)

rbf_model = svm.SVC(kernel='rbf',gamma=0.8, C=1)
t4 = time.time()
rbf_model.fit(X_train, y_train)
y_pred = rbf_model.predict(X_train)
accuracy = accuracy_score(y_train, y_pred)
print("rbf SVM Accuracy:", accuracy)
print("Training time 4:", time.time()-t4)

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

rf_model.fit(X_train, y_train)
t5 = time.time()
y_pred = rf_model.predict(X_train)
accuracy = accuracy_score(y_train, y_pred)
print("model fitting:", rf_model.score(X_train, y_train))
print("Random Forest Model Accuracy:", accuracy)
print("Training time 5:", time.time()-t5)

from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
t6 = time.time()
y_pred = knn_model.predict(X_train)
accuracy = accuracy_score(y_train, y_pred)
print("model fitting:", rf_model.score(X_train, y_train))
print("KNN Model Accuracy:", accuracy)
print("Training time 6:", time.time()-t6)

from sklearn.naive_bayes import GaussianNB
print(y_train.head())
print(X_train.head())
nb_model = GaussianNB()
t7 = time.time()
nb_model.fit(X_train, y_train)
y_pred = nb_model.predict(X_train)
accuracy = accuracy_score(y_train, y_pred)
print("model fitting:", rf_model.score(X_train, y_train))
print("Naive Bayes Model Accuracy:", accuracy)
print("Training time 7:", time.time()-t7)

import pickle


# Save the trained models to a file
pkl_file = "script.pkl"
with open(pkl_file, 'wb') as file:
    pickle.dump(bathrooms_mode, file)
    pickle.dump(bedrooms_mode, file)
    pickle.dump(longitude_mean, file)
    pickle.dump(latitude_mean, file)
    pickle.dump(cityname_mode, file)
    pickle.dump(state_mode, file)
    pickle.dump(preprocessing_body, file)
    pickle.dump(label_encoders, file)
    pickle.dump(y_encoder, file)
    pickle.dump(encoding_categorical_data, file)
    pickle.dump(cat_encoder, file)
    pickle.dump(scaler, file)
    pickle.dump(selector, file)
    pickle.dump(lin_model, file)
    pickle.dump(svm_model, file)
    pickle.dump(poly_svm, file)
    pickle.dump(rbf_model, file)
    pickle.dump(rf_model, file)
    pickle.dump(knn_model, file)
    pickle.dump(nb_model, file)
    pickle.dump(reverse_geocode, file)

"""#test script"""

# Load trained models from a file
with open(pkl_file,'rb') as file:
    pickled_bathrooms = pickle.load(file)
    pickled_bedrooms = pickle.load(file)
    pickled_longitude = pickle.load(file)
    pickled_latitude = pickle.load(file)
    pickled_cityname = pickle.load(file)
    pickled_state = pickle.load(file)
    preprocessing_body = pickle.load(file)
    pickled_label_encoders = pickle.load(file)
    pickled_y_encoder = pickle.load(file)
    pickled_encoding_categorical_data = pickle.load(file)
    pickled_cat_encoder = pickle.load(file)
    pickled_scaler = pickle.load(file)
    pickled_selector = pickle.load(file)
    pickled_lreg = pickle.load(file)
    pickled_svm = pickle.load(file)
    pickled_poly_svm = pickle.load(file)
    pickled_rbf_model = pickle.load(file)
    pickled_rf_model = pickle.load(file)
    pickled_knn_model = pickle.load(file)
    pickled_nb_model = pickle.load(file)
    pickled_reverse_geocode = pickle.load(file)

#geo

print(y_test.info())

X_test['bathrooms'].fillna(pickled_bathrooms, inplace=True)
X_test['bedrooms'].fillna(pickled_bedrooms, inplace=True)
X_test['longitude'].fillna(pickled_longitude, inplace=True)
X_test['latitude'].fillna(pickled_latitude, inplace=True)
X_test['cityname'].fillna(pickled_cityname, inplace=True)
X_test['state'].fillna(pickled_state, inplace=True)
X_test['pets_allowed'].fillna('none', inplace=True)
X_test['amenities'].fillna('none', inplace=True)

X_test.isnull().sum()

null_address_rows = X_test[X_test['address'].isnull()]

batch_size = 100
for i in range(0, len(null_address_rows), batch_size):
    batch_data = null_address_rows.iloc[i:i+batch_size]
    for index, row in batch_data.iterrows():
        latitude = row['latitude']
        longitude = row['longitude']
        address = pickled_reverse_geocode(latitude, longitude)
        if address:
            X_test.at[index, 'address'] = address
            print(f"New address added at index {index}: {address}")

preprocessing_body(X_test)
X_test.isnull().sum()

columns_to_label_encode = ['price_type', 'currency', 'category', 'fee']

for column in columns_to_label_encode:
    # pickled_label_encoders[column] = LabelEncoder()
    X_test[column] = pickled_label_encoders[column].transform(X_test[column])


y_test = pickled_y_encoder.transform(y_test)
y_test = pd.Series(y_test)
print(y_test)

X_test = pickled_encoding_categorical_data(X_test)

print(y_test.head())
print(X_test.head())
X_test.reset_index(drop=True, inplace=True)
y_test.reset_index(drop=True, inplace=True)

columns_to_target_encode = ['state', 'cityname', 'body', 'title', 'address']
encoded_data = pickled_cat_encoder.transform(X_test[columns_to_target_encode])
X_test[columns_to_target_encode] = encoded_data
print(y_test.head())
print(X_test.head())

print(y_test.head())
print(X_test.head())

X_selected = pickled_selector.transform(X_test)
selected_feature_indices = pickled_selector.get_support(indices=True)
selected_feature_names = X_test.columns[selected_feature_indices]
X_test = X_test[selected_feature_names]
print(y_test.head())
print(X_test.head())

print(y_test.head())
print(X_test.head())

pickled_scaler.transform(X_test)
X_scaled = pickled_scaler.transform(X_test)
X_test = pd.DataFrame(X_scaled, columns=X_train.columns)



y_pred = pickled_lreg.predict(X_test)

start_time = time.time()

# Your model testing code here

# Calculate the test time

accuracy = accuracy_score(y_test, y_pred)
print("Logistic Regression Model Accuracy:", accuracy)
test_time = time.time() - start_time
print("Test time:", test_time, "seconds")

start_time = time.time()
y_pred = pickled_svm.predict(X_test)
accuracy_Linear_SVM = accuracy_score(y_test, y_pred)
print("Linear SVM Model Accuracy:",  accuracy_Linear_SVM)
test_time_Linear_SVM = time.time() - start_time
print("Test time:", test_time_Linear_SVM, "seconds")

start_time = time.time()
y_pred = pickled_poly_svm.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("poly SVM Model Accuracy:", accuracy)
test_time = time.time() - start_time
print("Test time:", test_time, "seconds")

start_time = time.time()
y_pred = pickled_rbf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("rbf SVM Model Accuracy:", accuracy)
test_time = time.time() - start_time
print("Test time:", test_time, "seconds")

start_time = time.time()
y_pred = pickled_rf_model.predict(X_test)
accuracy_Random_Forest= accuracy_score(y_test, y_pred)
print("Random Forest Model Accuracy:", accuracy_Random_Forest)
test_time_Random_Forest = time.time() - start_time
print("Test time:", test_time_Random_Forest, "seconds")

start_time = time.time()
y_pred = pickled_knn_model.predict(X_test)
accuracy_KNN_Model= accuracy_score(y_test, y_pred)
print("KNN Model Accuracy:",accuracy_KNN_Model)
test_time_KNN_Model = time.time() - start_time
print("Test time:",test_time_KNN_Model, "seconds")

start_time = time.time()
y_pred = pickled_nb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Naive Bayes Model Accuracy:", accuracy)
test_time = time.time() - start_time
print("Test time:", test_time, "seconds")

import matplotlib.pyplot as plt

# Store model names
model_names = ['Random Forest', 'KNN','Linear SVM']

# Store accuracy, training time, and test time for each model
accuracies = [accuracy_Random_Forest, accuracy_KNN_Model, accuracy_Linear_SVM]
training_times = [t5, t6, t2]
test_times = [test_time_Random_Forest, test_time_KNN_Model, test_time_Linear_SVM]

# Plotting
fig, axes = plt.subplots(3, 1, figsize=(5, 10))

# Plot classification accuracy
axes[0].bar(model_names, accuracies, color='skyblue')
axes[0].set_ylabel('Accuracy')
axes[0].set_title('Classification Accuracy')

# Plot total training time
axes[1].bar(model_names, training_times, color='lightgreen')
axes[1].set_ylabel('Training Time (seconds)')
axes[1].set_title('Total Training Time')

# Plot total test time
axes[2].bar(model_names, test_times, color='salmon')
axes[2].set_ylabel('Test Time (seconds)')
axes[2].set_title('Total Test Time')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()